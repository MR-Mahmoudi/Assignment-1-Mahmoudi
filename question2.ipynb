{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "oT7eb_5fqdT3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Pandas display options\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 10)\n",
        "from IPython.core.display import display, HTML"
      ],
      "metadata": {
        "id": "IiwaBQOEaDZI"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def force_show_all(df):\n",
        "  \"Print dataframe nicely\"\n",
        "  with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None):\n",
        "    display(HTML(df.to_html()))"
      ],
      "metadata": {
        "id": "lRdkNiBsaRxc"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data= pd.read_csv(\"/content/Assignment1_Q2_Data.csv\")\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "uWrmeBgUqkDp",
        "outputId": "9cebb81b-cc11-4680-991c-7a39a22d071f"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    House ID  Local Price  Bathrooms  Land Area  Living area  # Garages  \\\n",
              "0          1      4.918        1.000      3.472      0.998        1.000   \n",
              "1          2      5.021        1.000      3.531      1.500        2.000   \n",
              "2          3      4.543        1.000      2.275      1.175        1.000   \n",
              "3          4      4.557        1.000      4.050      1.232        1.000   \n",
              "4          5      5.060        1.000      4.455      1.121        1.000   \n",
              "..       ...        ...          ...        ...        ...          ...   \n",
              "23        24      6.093        1.500      6.726      1.652        1.000   \n",
              "24        25      8.361        1.500      9.150      1.777        2.000   \n",
              "25        26      8.140        1.000      8.000      1.504        2.000   \n",
              "26        27      9.142        1.500      7.326      1.831        1.500   \n",
              "27        28     12.000        1.500      5.000      1.200        2.000   \n",
              "\n",
              "    # Rooms  # Bedrooms  Age of home  Construction type  Architecture type  \\\n",
              "0         7          4          42            3                  1           \n",
              "1         7          4          62            1                  1           \n",
              "2         6          3          40            2                  1           \n",
              "3         6          3          54            4                  1           \n",
              "4         6          3          42            3                  1           \n",
              "..      ...        ...         ...          ...                ...           \n",
              "23        6          3          44            4                  1           \n",
              "24        8          4          48            1                  1           \n",
              "25        7          3           3            1                  3           \n",
              "26        8          4          31            4                  1           \n",
              "27        6          3          30            3                  1           \n",
              "\n",
              "    # Fire places  Price (1000$)  \n",
              "0           0         25.900      \n",
              "1           0         29.500      \n",
              "2           0         27.900      \n",
              "3           0         25.900      \n",
              "4           0         29.900      \n",
              "..        ...            ...      \n",
              "23          0         37.900      \n",
              "24          1         38.900      \n",
              "25          0         36.900      \n",
              "26          0         45.800      \n",
              "27          1         41.000      \n",
              "\n",
              "[28 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9da79a1-aadd-40c6-8195-1526730863a4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>House ID</th>\n",
              "      <th>Local Price</th>\n",
              "      <th>Bathrooms</th>\n",
              "      <th>Land Area</th>\n",
              "      <th>Living area</th>\n",
              "      <th># Garages</th>\n",
              "      <th># Rooms</th>\n",
              "      <th># Bedrooms</th>\n",
              "      <th>Age of home</th>\n",
              "      <th>Construction type</th>\n",
              "      <th>Architecture type</th>\n",
              "      <th># Fire places</th>\n",
              "      <th>Price (1000$)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>4.918</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.472</td>\n",
              "      <td>0.998</td>\n",
              "      <td>1.000</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>25.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>5.021</td>\n",
              "      <td>1.000</td>\n",
              "      <td>3.531</td>\n",
              "      <td>1.500</td>\n",
              "      <td>2.000</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>62</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>29.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4.543</td>\n",
              "      <td>1.000</td>\n",
              "      <td>2.275</td>\n",
              "      <td>1.175</td>\n",
              "      <td>1.000</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>40</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>27.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4.557</td>\n",
              "      <td>1.000</td>\n",
              "      <td>4.050</td>\n",
              "      <td>1.232</td>\n",
              "      <td>1.000</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>54</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>25.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5.060</td>\n",
              "      <td>1.000</td>\n",
              "      <td>4.455</td>\n",
              "      <td>1.121</td>\n",
              "      <td>1.000</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>42</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>29.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>6.093</td>\n",
              "      <td>1.500</td>\n",
              "      <td>6.726</td>\n",
              "      <td>1.652</td>\n",
              "      <td>1.000</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>44</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>37.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>8.361</td>\n",
              "      <td>1.500</td>\n",
              "      <td>9.150</td>\n",
              "      <td>1.777</td>\n",
              "      <td>2.000</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>26</td>\n",
              "      <td>8.140</td>\n",
              "      <td>1.000</td>\n",
              "      <td>8.000</td>\n",
              "      <td>1.504</td>\n",
              "      <td>2.000</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>36.900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>9.142</td>\n",
              "      <td>1.500</td>\n",
              "      <td>7.326</td>\n",
              "      <td>1.831</td>\n",
              "      <td>1.500</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>45.800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>28</td>\n",
              "      <td>12.000</td>\n",
              "      <td>1.500</td>\n",
              "      <td>5.000</td>\n",
              "      <td>1.200</td>\n",
              "      <td>2.000</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>30</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>41.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9da79a1-aadd-40c6-8195-1526730863a4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f9da79a1-aadd-40c6-8195-1526730863a4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f9da79a1-aadd-40c6-8195-1526730863a4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 28 rows and 13 columns and the ID is not our feature. so we need to get rid of it. So it leaves us with 11 features."
      ],
      "metadata": {
        "id": "6vKyW_smllLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(data))  # num_data x num_features \n",
        "data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq91P1DH7e7P",
        "outputId": "bcda3118-3778-4d06-92b5-f10f80845575"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 13)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['House ID', 'Local Price', 'Bathrooms', 'Land Area', 'Living area',\n",
              "       '# Garages', '# Rooms', '# Bedrooms', 'Age of home',\n",
              "       'Construction type', 'Architecture type', '# Fire places',\n",
              "       'Price (1000$)'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(x_train, y_train, curr_theta):\n",
        "\t'''\n",
        "\tx_train: [2xnum_data]\n",
        "\ty_train: [1xnum_data]\n",
        "\t'''\n",
        "\tnum_data = np.shape(y_train)[1]\n",
        "\tassert np.shape(x_train)[1] == np.shape(y_train)[1]\n",
        "\n",
        "\terror = y_train - np.matmul(curr_theta, x_train)  # 1 x num_data \n",
        "\tcost = np.sum(error**2) / (2 * num_data)\n",
        "\treturn cost"
      ],
      "metadata": {
        "id": "poFFiR7GqsyZ"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(x_train, y_train, curr_theta, learning_rate=0.1, num_steps=1000, thresh: float=1e-5, do_print=True):\n",
        "\t'''\n",
        "\tx_train: [num_features, num_data]\n",
        "\ty_train: [1, num_data]\n",
        "\tcurr_theta: [1, num_features]\n",
        "\t'''\n",
        "\tassert np.shape(x_train)[1] == np.shape(y_train)[1]\n",
        "\n",
        "\tnum_data = np.shape(y_train)[1]\n",
        "\tloss_per_step = []\n",
        "\t\n",
        "\tfor step in range(num_steps):\n",
        "\n",
        "\t\tcurr_predict = np.matmul(curr_theta, x_train)  # 1 x num_data\n",
        "\t\terror = (y_train - curr_predict).T  # num_data x 1\n",
        "\t\tassert np.shape(curr_predict) == np.shape(y_train)\n",
        "\n",
        "\t\tif np.linalg.norm(error) < thresh:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t\td_cost_d_theta = (1/num_data) * (np.matmul(x_train, error)).T # 1 x num_features\n",
        "\t\tnew_theta = curr_theta + (learning_rate) * d_cost_d_theta\n",
        "\n",
        "\t\tnew_loss = loss_function(x_train, y_train, new_theta)\n",
        "\t\tif do_print:\n",
        "\t\t\tprint(f'step = {step}, curr_theta = {curr_theta}, new_theta={new_theta}, new_loss = {new_loss}')\n",
        "\t\tloss_per_step.append(new_loss)\n",
        "\n",
        "\t\tif step >= 2:\n",
        "\t\t\tif (np.abs(loss_per_step[-1] - loss_per_step[-2]) < thresh):\n",
        "\t\t\t\tbreak\n",
        "\t\n",
        "\t\t# update theta \n",
        "\t\tcurr_theta = new_theta\n",
        "\n",
        "\treturn new_theta, loss_per_step"
      ],
      "metadata": {
        "id": "HE_cLUf5q22I"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We have to normalize our data. Otherwise, we will not be able to find their influence on our data set"
      ],
      "metadata": {
        "id": "LP4lWs9VmSgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbRt_tUmY1BR",
        "outputId": "9e86749a-b6c6-46ef-e450-9580910d8de2"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "House ID            14.500\n",
              "Local Price          7.222\n",
              "Bathrooms            1.268\n",
              "Land Area            6.461\n",
              "Living area          1.512\n",
              "                     ...  \n",
              "Age of home         36.321\n",
              "Construction type    2.250\n",
              "Architecture type    1.214\n",
              "# Fire places        0.321\n",
              "Price (1000$)       38.157\n",
              "Length: 13, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.copy()\n",
        "df = df.drop('House ID', axis=1)\n",
        "normalized_df = (df - df.mean()) / df.std()\n",
        "normalized_df = (df - df.min()) / df.max()\n"
      ],
      "metadata": {
        "id": "pvnc7ly6Zoyh"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "force_show_all(normalized_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "wcgazSaUaWqZ",
        "outputId": "9fe7f427-5fb9-479d-f57e-a05f1066aec9"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Local Price</th>\n",
              "      <th>Bathrooms</th>\n",
              "      <th>Land Area</th>\n",
              "      <th>Living area</th>\n",
              "      <th># Garages</th>\n",
              "      <th># Rooms</th>\n",
              "      <th># Bedrooms</th>\n",
              "      <th>Age of home</th>\n",
              "      <th>Construction type</th>\n",
              "      <th>Architecture type</th>\n",
              "      <th># Fire places</th>\n",
              "      <th>Price (1000$)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.063</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.629</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.069</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.154</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.952</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.040</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.041</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.823</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.071</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.629</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.855</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.122</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.279</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.774</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.104</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.566</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.763</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.588</td>\n",
              "      <td>0.715</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.629</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.644</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.822</td>\n",
              "      <td>0.592</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.118</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.073</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.086</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.212</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.435</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.145</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.435</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.126</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.343</td>\n",
              "      <td>0.043</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.071</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.694</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.104</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.566</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.265</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.201</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.758</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.171</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.361</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.306</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.237</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.377</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.313</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.432</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.128</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.082</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.222</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.306</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.299</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.595</td>\n",
              "      <td>0.247</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.758</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.134</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.661</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.272</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.537</td>\n",
              "      <td>0.235</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.726</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.259</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.447</td>\n",
              "      <td>0.155</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.320</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.395</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.452</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.494</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.066</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.435</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.178</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen, we normalized our data. Also we droped the ID column which was not important for our calculation."
      ],
      "metadata": {
        "id": "m3TghrDPmqHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = normalized_df.to_numpy().T\n",
        "print(np.shape(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI1rmMWDatl9",
        "outputId": "a75228f0-6e9a-43ae-e32e-168581ad841c"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It shows that we have 12 features and 28 data. But we have to notice that one of the rows is price which is our y. "
      ],
      "metadata": {
        "id": "ocD9V504m3rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(dataset))\n",
        "print(np.shape(dataset)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrQU-NLGdEde",
        "outputId": "783945ce-8d55-42b5-c441-301edbb16c55"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12, 28)\n",
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We need to devide our data set to train and test. I chose to train 80% of my dataset."
      ],
      "metadata": {
        "id": "n2N1BRLpnNLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = np.shape(dataset)[0]\n",
        "num_data = np.shape(dataset)[1]\n",
        "\n",
        "num_training = int(0.8 * num_data)\n",
        "print(f'num_training = {num_training}, num_test = {num_data-num_training}')\n",
        "train_dataset = dataset[:, :num_training]\n",
        "test_dataset = dataset[:, num_training:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XppvzCXgcKWV",
        "outputId": "361170a0-570a-461c-dc65-e55c19d715ce"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_training = 22, num_test = 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train_dataset = {np.shape(train_dataset)}')\n",
        "x_train = train_dataset[:-1, :]\n",
        "y_train = train_dataset[-1, :]\n",
        "y_train = y_train[np.newaxis, :]\n",
        "print(np.shape(x_train), np.shape(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arIyvgf7foir",
        "outputId": "67bc8bec-f2fc-42db-f02b-28550d694cbf"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset = (12, 22)\n",
            "(11, 22) (1, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We devided our features from price which is our output or y.\n",
        "So, we have 11 features corresponded to y.\n",
        "and as we chose 80%, the x-train and y-train are 22."
      ],
      "metadata": {
        "id": "uZYMcB_xnalb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Can you use only this feature to predict the price?"
      ],
      "metadata": {
        "id": "BJ3fcNHAjCli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'test_dataset = {np.shape(test_dataset)}')\n",
        "x_test = test_dataset[:-1, :]\n",
        "y_test = test_dataset[-1, :]\n",
        "y_test = y_test[np.newaxis, :]\n",
        "print(np.shape(x_test), np.shape(y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq_BRpA2fvAK",
        "outputId": "03eaf575-a8ba-45b4-9aa3-c5bde319e758"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_dataset = (12, 6)\n",
            "(11, 6) (1, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 6 number of data as our test data."
      ],
      "metadata": {
        "id": "kr7FlEgdn1t7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_training = np.shape(x_train)[1]\n",
        "\n",
        "# to make it easier to work with theta0\n",
        "x_train_bias =  np.vstack((np.ones((1, num_training)), x_train))  # num_features x num_data\n",
        "print(f'x_train_bias = {np.shape(x_train_bias)}')\n",
        "\n",
        "num_features = np.shape(x_train_bias)[0]\n",
        "init_theta = np.zeros((1, num_features))\n",
        "learning_rate = 0.1\n",
        "num_steps = 5000\n",
        "\n",
        "final_theta, loss_history = gradient_descent(x_train_bias, y_train, init_theta, learning_rate, num_steps)\n",
        "plt.figure(); plt.plot(loss_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "co1DUIs-q7HI",
        "outputId": "9c79c283-cec1-4236-fbb0-3b62493c75f6"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train_bias = (12, 22)\n",
            "step = 0, curr_theta = [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], new_theta=[[0.01357747 0.00566342 0.00430025 0.00655685 0.00495726 0.0111816\n",
            "  0.00383339 0.00525752 0.00599221 0.00514911 0.00083342 0.00817539]], new_loss = 0.02050765430679732\n",
            "step = 1, curr_theta = [[0.01357747 0.00566342 0.00430025 0.00655685 0.00495726 0.0111816\n",
            "  0.00383339 0.00525752 0.00599221 0.00514911 0.00083342 0.00817539]], new_theta=[[0.02379947 0.01059413 0.00816563 0.01201624 0.00929592 0.02009425\n",
            "  0.00705735 0.00959076 0.01015697 0.0091477  0.00148629 0.01513522]], new_loss = 0.01704980742536422\n",
            "step = 2, curr_theta = [[0.02379947 0.01059413 0.00816563 0.01201624 0.00929592 0.02009425\n",
            "  0.00705735 0.00959076 0.01015697 0.0091477  0.00148629 0.01513522]], new_theta=[[0.03140134 0.01494296 0.01168209 0.01661047 0.01314274 0.02722696\n",
            "  0.0098002  0.01319683 0.01289983 0.01224462 0.00199894 0.02112642]], new_loss = 0.014784256895761667\n",
            "step = 3, curr_theta = [[0.03140134 0.01494296 0.01168209 0.01661047 0.01314274 0.02722696\n",
            "  0.0098002  0.01319683 0.01289983 0.01224462 0.00199894 0.02112642]], new_theta=[[0.036959   0.01882823 0.01491712 0.0205216  0.01659715 0.03296287\n",
            "  0.01216252 0.01623023 0.01453829 0.01463487 0.00240292 0.02634286]], new_loss = 0.013255992112757532\n",
            "step = 4, curr_theta = [[0.036959   0.01882823 0.01491712 0.0205216  0.01659715 0.03296287\n",
            "  0.01216252 0.01623023 0.01453829 0.01463487 0.00240292 0.02634286]], new_theta=[[0.04092342 0.02234278 0.01792374 0.02389238 0.0197372  0.03760216\n",
            "  0.01422317 0.01881201 0.01532094 0.0164712  0.00272292 0.0309368 ]], new_loss = 0.012185524642523664\n",
            "step = 5, curr_theta = [[0.04092342 0.02234278 0.01792374 0.02389238 0.0197372  0.03760216\n",
            "  0.01422317 0.01881201 0.01532094 0.0164712  0.00272292 0.0309368 ]], new_theta=[[0.04364774 0.02555947 0.02074366 0.02683474 0.02262415 0.04138003\n",
            "  0.01604403 0.02103709 0.01544233 0.01787335 0.00297824 0.03502793]], new_loss = 0.011401102014268666\n",
            "step = 6, curr_theta = [[0.04364774 0.02555947 0.02074366 0.02683474 0.02262415 0.04138003\n",
            "  0.01604403 0.02103709 0.01544233 0.01787335 0.00297824 0.03502793]], new_theta=[[0.04540846 0.02853554 0.0234097  0.02943645 0.02530613 0.04448077\n",
            "  0.01767359 0.02297984 0.01505469 0.01893511 0.00318398 0.03871037]], new_loss = 0.010797103678865631\n",
            "step = 7, curr_theta = [[0.04540846 0.02853554 0.0234097  0.02943645 0.02530613 0.04448077\n",
            "  0.01767359 0.02297984 0.01505469 0.01893511 0.00318398 0.03871037]], new_theta=[[0.046422   0.03131595 0.02594769 0.03176636 0.02782097 0.04704879\n",
            "  0.01914995 0.02469861 0.01427712 0.01973002 0.00335192 0.04205824]], new_loss = 0.010308506228079307\n",
            "step = 8, curr_theta = [[0.046422   0.03131595 0.02594769 0.03176636 0.02782097 0.04704879\n",
            "  0.01914995 0.02469861 0.01427712 0.01973002 0.00335192 0.04205824]], new_theta=[[0.04685776 0.03393608 0.02837806 0.03387852 0.03019842 0.04919723\n",
            "  0.020503   0.02623913 0.01320275 0.02031569 0.00349127 0.04512996]], new_loss = 0.00989521110150545\n",
            "step = 9, curr_theta = [[0.04685776 0.03393608 0.02837806 0.03387852 0.03019842 0.04919723\n",
            "  0.020503   0.02623913 0.01320275 0.02031569 0.00349127 0.04512996]], new_theta=[[0.04684827 0.03642375 0.03071692 0.03581534 0.03246192 0.05101479\n",
            "  0.02175621 0.02763727 0.01190437 0.02073729 0.0036092  0.04797165]], new_loss = 0.009532425388457267\n",
            "step = 10, curr_theta = [[0.04684827 0.03642375 0.03071692 0.03581534 0.03246192 0.05101479\n",
            "  0.02175621 0.02763727 0.01190437 0.02073729 0.0036092  0.04797165]], new_theta=[[0.04649722 0.03880091 0.03297704 0.03761015 0.03462995 0.05257094\n",
            "  0.02292803 0.02892119 0.01043884 0.02103021 0.0037113  0.05061977]], new_loss = 0.009204757718376323\n",
            "step = 11, curr_theta = [[0.04649722 0.03880091 0.03297704 0.03761015 0.03462995 0.05257094\n",
            "  0.02292803 0.02892119 0.01043884 0.02103021 0.0037113  0.05061977]], new_theta=[[0.04588567 0.04108487 0.03516857 0.03928912 0.03671708 0.05392018\n",
            "  0.02403298 0.03011299 0.00885056 0.02122223 0.00380193 0.05310324]], new_loss = 0.008902594293221778\n",
            "step = 12, curr_theta = [[0.04588567 0.04108487 0.03516857 0.03928912 0.03671708 0.05392018\n",
            "  0.02403298 0.03011299 0.00885056 0.02122223 0.00380193 0.05310324]], new_theta=[[0.04507699 0.04328931 0.03729957 0.04087287 0.03873488 0.05510519\n",
            "  0.02508249 0.03123001 0.00717413 0.02133509 0.00388446 0.05544501]], new_loss = 0.00861987436615939\n",
            "step = 13, curr_theta = [[0.04507699 0.04328931 0.03729957 0.04087287 0.03873488 0.05510519\n",
            "  0.02508249 0.03123001 0.00717413 0.02133509 0.00388446 0.05544501]], new_theta=[[0.04412066 0.04542508 0.03937651 0.0423776  0.04069248 0.05615946\n",
            "  0.02608557 0.0322859  0.00543653 0.02138588 0.00396152 0.05766342]], new_loss = 0.008352724641044064\n",
            "step = 14, curr_theta = [[0.04412066 0.04542508 0.03937651 0.0423776  0.04069248 0.05615946\n",
            "  0.02608557 0.0322859  0.00543653 0.02138588 0.00396152 0.05766342]], new_theta=[[0.04305531 0.04750078 0.04140457 0.0438161  0.04259717 0.05710924\n",
            "  0.02704933 0.03329138 0.00365873 0.02138799 0.00403513 0.05977311]], new_loss = 0.008098620845846479\n",
            "step = 15, curr_theta = [[0.04305531 0.04750078 0.04140457 0.0438161  0.04259717 0.05710924\n",
            "  0.02704933 0.03329138 0.00365873 0.02138799 0.00403513 0.05977311]], new_theta=[[0.04191106 0.04952329 0.04338793 0.04519847 0.04445476 0.05797513\n",
            "  0.0279794  0.03425488 0.00185702 0.02135194 0.00410684 0.06178588]], new_loss = 0.00785587287060231\n",
            "step = 16, curr_theta = [[0.04191106 0.04952329 0.04338793 0.04519847 0.04445476 0.05797513\n",
            "  0.0279794  0.03425488 0.00185702 0.02135194 0.00410684 0.06178588]], new_theta=[[4.07113497e-02 5.14980835e-02 4.53300045e-02 4.65327025e-02\n",
            "  4.62698972e-02 5.87732872e-02 2.88802323e-02 3.51830673e-02\n",
            "  4.40055236e-05 2.12859595e-02 4.17785147e-03 6.37112552e-02]], new_loss = 0.0076233085045277395\n",
            "step = 17, curr_theta = [[4.07113497e-02 5.14980835e-02 4.53300045e-02 4.65327025e-02\n",
            "  4.62698972e-02 5.87732872e-02 2.88802323e-02 3.51830673e-02\n",
            "  4.40055236e-05 2.12859595e-02 4.17785147e-03 6.37112552e-02]], new_theta=[[ 0.03947443  0.05342959  0.04723357  0.04782512  0.04804634  0.05951639\n",
            "   0.02975538  0.03608118 -0.00177057  0.02119655  0.00424907  0.06555699]], new_loss = 0.007400079074576488\n",
            "step = 18, curr_theta = [[ 0.03947443  0.05342959  0.04723357  0.04782512  0.04804634  0.05951639\n",
            "   0.02975538  0.03608118 -0.00177057  0.02119655  0.00424907  0.06555699]], new_theta=[[ 0.03821446  0.05532136  0.04910094  0.04908075  0.04978712  0.06021441\n",
            "   0.03060766  0.03695338 -0.00357921  0.0210888   0.00432119  0.06732945]], new_loss = 0.007185539911880549\n",
            "step = 19, curr_theta = [[ 0.03821446  0.05532136  0.04910094  0.04908075  0.04978712  0.06021441\n",
            "   0.03060766  0.03695338 -0.00357921  0.0210888   0.00432119  0.06732945]], new_theta=[[ 0.0369424   0.05717631  0.05093401  0.05030361  0.05149472  0.06087517\n",
            "   0.03143935  0.03780295 -0.00537619  0.02096671  0.00439472  0.06903388]], new_loss = 0.006979176754300717\n",
            "step = 20, curr_theta = [[ 0.0369424   0.05717631  0.05093401  0.05030361  0.05149472  0.06087517\n",
            "   0.03143935  0.03780295 -0.00537619  0.02096671  0.00439472  0.06903388]], new_theta=[[ 0.0356667   0.05899681  0.0527344   0.05149693  0.05317117  0.06150483\n",
            "   0.03225228  0.0386325  -0.00715714  0.02083345  0.00447005  0.07067468]], new_loss = 0.006780560352361287\n",
            "step = 21, curr_theta = [[ 0.0356667   0.05899681  0.0527344   0.05149693  0.05317117  0.06150483\n",
            "   0.03225228  0.0386325  -0.00715714  0.02083345  0.00447005  0.07067468]], new_theta=[[ 0.03439387  0.06078482  0.05450349  0.05266328  0.05481816  0.06210824\n",
            "   0.0330479   0.03944411 -0.00891879  0.0206915   0.00454743  0.07225555]], new_loss = 0.006589318394702768\n",
            "step = 22, curr_theta = [[ 0.03439387  0.06078482  0.05450349  0.05266328  0.05481816  0.06210824\n",
            "   0.0330479   0.03944411 -0.00891879  0.0206915   0.00454743  0.07225555]], new_theta=[[ 0.03312887  0.06254199  0.05624245  0.05380477  0.0564371   0.06268921\n",
            "   0.03382744  0.04023945 -0.01065869  0.02054282  0.00462707  0.07377964]], new_loss = 0.006405118072677515\n",
            "step = 23, curr_theta = [[ 0.03312887  0.06254199  0.05624245  0.05380477  0.0564371   0.06268921\n",
            "   0.03382744  0.04023945 -0.01065869  0.02054282  0.00462707  0.07377964]], new_theta=[[ 0.0318755   0.06426969  0.05795233  0.0549231   0.05802921  0.06325077\n",
            "   0.03459187  0.04101987 -0.01237504  0.02038896  0.0047091   0.07524969]], new_loss = 0.006227655183614564\n",
            "step = 24, curr_theta = [[ 0.0318755   0.06426969  0.05795233  0.0549231   0.05802921  0.06325077\n",
            "   0.03459187  0.04101987 -0.01237504  0.02038896  0.0047091   0.07524969]], new_theta=[[ 0.03063658  0.06596912  0.05963404  0.05601968  0.0595955   0.06379531\n",
            "   0.03534202  0.04178644 -0.01406656  0.02023114  0.00479359  0.07666805]], new_loss = 0.006056647255706244\n",
            "step = 25, curr_theta = [[ 0.03063658  0.06596912  0.05963404  0.05601968  0.0595955   0.06379531\n",
            "   0.03534202  0.04178644 -0.01406656  0.02023114  0.00479359  0.07666805]], new_theta=[[ 0.02941424  0.06764128  0.06128838  0.05709567  0.06113686  0.06432471\n",
            "   0.03607857  0.04254005 -0.01573239  0.02007036  0.00488059  0.07803685]], new_loss = 0.005891829149310521\n",
            "step = 26, curr_theta = [[ 0.02941424  0.06764128  0.06128838  0.05709567  0.06113686  0.06432471\n",
            "   0.03607857  0.04254005 -0.01573239  0.02007036  0.00488059  0.07803685]], new_theta=[[ 0.02821001  0.06928708  0.0629161   0.05815205  0.06265409  0.06484049\n",
            "   0.0368021   0.04328142 -0.01737193  0.01990737  0.0049701   0.07935795]], new_loss = 0.005732950185944854\n",
            "step = 27, curr_theta = [[ 0.02821001  0.06928708  0.0629161   0.05815205  0.06265409  0.06484049\n",
            "   0.0368021   0.04328142 -0.01737193  0.01990737  0.0049701   0.07935795]], new_theta=[[ 0.02702499  0.07090729  0.06451788  0.05918964  0.06414788  0.06534384\n",
            "   0.03751311  0.04401114 -0.01898486  0.0197428   0.00506211  0.08063306]], new_loss = 0.005579772222352278\n",
            "step = 28, curr_theta = [[ 0.02702499  0.07090729  0.06451788  0.05918964  0.06414788  0.06534384\n",
            "   0.03751311  0.04401114 -0.01898486  0.0197428   0.00506211  0.08063306]], new_theta=[[ 0.02585995  0.07250263  0.06609434  0.06020915  0.06561885  0.06583573\n",
            "   0.03821203  0.04472973 -0.02057102  0.01957715  0.00515661  0.08186374]], new_loss = 0.005432068311731682\n",
            "step = 29, curr_theta = [[ 0.02585995  0.07250263  0.06609434  0.06020915  0.06561885  0.06583573\n",
            "   0.03821203  0.04472973 -0.02057102  0.01957715  0.00515661  0.08186374]], new_theta=[[ 0.02471536  0.07407374  0.06764605  0.06121118  0.06706758  0.06631694\n",
            "   0.03889923  0.04543761 -0.02213042  0.01941081  0.00525354  0.08305142]], new_loss = 0.00528962173215445\n",
            "step = 30, curr_theta = [[ 0.02471536  0.07407374  0.06764605  0.06121118  0.06706758  0.06631694\n",
            "   0.03889923  0.04543761 -0.02213042  0.01941081  0.00525354  0.08305142]], new_theta=[[ 0.0235915   0.07562121  0.06917358  0.06219629  0.06849459  0.06678809\n",
            "   0.03957506  0.04613513 -0.02366315  0.01924412  0.00535285  0.08419744]], new_loss = 0.005152225246857916\n",
            "step = 31, curr_theta = [[ 0.0235915   0.07562121  0.06917358  0.06219629  0.06849459  0.06678809\n",
            "   0.03957506  0.04613513 -0.02366315  0.01924412  0.00535285  0.08419744]], new_theta=[[ 0.02248851  0.07714558  0.07067744  0.06316494  0.06990038  0.06724971\n",
            "   0.04023981  0.04682261 -0.02516941  0.01907732  0.0054545   0.08530304]], new_loss = 0.005019680513083965\n",
            "step = 32, curr_theta = [[ 0.02248851  0.07714558  0.07067744  0.06316494  0.06990038  0.06724971\n",
            "   0.04023981  0.04682261 -0.02516941  0.01907732  0.0054545   0.08530304]], new_theta=[[ 0.02140636  0.07864736  0.07215811  0.06411756  0.07128541  0.06770221\n",
            "   0.04089376  0.04750033 -0.02664944  0.01891064  0.00555842  0.08636942]], new_loss = 0.004891797588045296\n",
            "step = 33, curr_theta = [[ 0.02140636  0.07864736  0.07215811  0.06411756  0.07128541  0.06770221\n",
            "   0.04089376  0.04750033 -0.02664944  0.01891064  0.00555842  0.08636942]], new_theta=[[ 0.02034496  0.08012703  0.07361609  0.06505456  0.07265009  0.06814595\n",
            "   0.04153716  0.04816852 -0.02810355  0.01874426  0.00566454  0.0873977 ]], new_loss = 0.0047683945002007\n",
            "step = 34, curr_theta = [[ 0.02034496  0.08012703  0.07361609  0.06505456  0.07265009  0.06814595\n",
            "   0.04153716  0.04816852 -0.02810355  0.01874426  0.00566454  0.0873977 ]], new_theta=[[ 0.01930414  0.08158504  0.07505181  0.06597628  0.07399486  0.06858124\n",
            "   0.04217025  0.04882741 -0.02953206  0.01857833  0.00577279  0.08838896]], new_loss = 0.004649296866061462\n",
            "step = 35, curr_theta = [[ 0.01930414  0.08158504  0.07505181  0.06597628  0.07399486  0.06858124\n",
            "   0.04217025  0.04882741 -0.02953206  0.01857833  0.00577279  0.08838896]], new_theta=[[ 0.01828367  0.08302183  0.07646572  0.06688306  0.0753201   0.06900832\n",
            "   0.04279325  0.04947721 -0.03093534  0.01841296  0.00588311  0.08934422]], new_loss = 0.004534337540151443\n",
            "step = 36, curr_theta = [[ 0.01828367  0.08302183  0.07646572  0.06688306  0.0753201   0.06900832\n",
            "   0.04279325  0.04947721 -0.03093534  0.01841296  0.00588311  0.08934422]], new_theta=[[ 0.0172833   0.08443781  0.07785824  0.06777522  0.07662618  0.06942741\n",
            "   0.04340636  0.05011808 -0.03231377  0.01824826  0.00599542  0.09026447]], new_loss = 0.004423356290296288\n",
            "step = 37, curr_theta = [[ 0.0172833   0.08443781  0.07785824  0.06777522  0.07662618  0.06942741\n",
            "   0.04340636  0.05011808 -0.03231377  0.01824826  0.00599542  0.09026447]], new_theta=[[ 0.01630271  0.08583339  0.07922979  0.06865306  0.07791348  0.06983871\n",
            "   0.04400978  0.05075021 -0.03366773  0.01808433  0.00610966  0.09115067]], new_loss = 0.0043161994932213515\n",
            "step = 38, curr_theta = [[ 0.01630271  0.08583339  0.07922979  0.06865306  0.07791348  0.06983871\n",
            "   0.04400978  0.05075021 -0.03366773  0.01808433  0.00610966  0.09115067]], new_theta=[[ 0.01534159  0.08720895  0.08058075  0.06951684  0.07918234  0.07024238\n",
            "   0.04460371  0.05137375 -0.03499763  0.01792124  0.00622576  0.09200374]], new_loss = 0.004212719847168644\n",
            "step = 39, curr_theta = [[ 0.01534159  0.08720895  0.08058075  0.06951684  0.07918234  0.07024238\n",
            "   0.04460371  0.05137375 -0.03499763  0.01792124  0.00622576  0.09200374]], new_theta=[[ 0.01439962  0.08856486  0.08191154  0.07036685  0.08043311  0.07063858\n",
            "   0.04518831  0.05198886 -0.03630387  0.01775906  0.00634364  0.09282458]], new_loss = 0.004112776099314468\n",
            "step = 40, curr_theta = [[ 0.01439962  0.08856486  0.08191154  0.07036685  0.08043311  0.07063858\n",
            "   0.04518831  0.05198886 -0.03630387  0.01775906  0.00634364  0.09282458]], new_theta=[[ 0.01347644  0.08990148  0.08322251  0.07120334  0.08166612  0.07102745\n",
            "   0.04576377  0.05259568 -0.03758685  0.01759785  0.00646325  0.09361405]], new_loss = 0.0040162327864357575\n",
            "step = 41, curr_theta = [[ 0.01347644  0.08990148  0.08322251  0.07120334  0.08166612  0.07102745\n",
            "   0.04576377  0.05259568 -0.03758685  0.01759785  0.00646325  0.09361405]], new_theta=[[ 0.01257171  0.09121918  0.08451405  0.07202657  0.08288169  0.07140912\n",
            "   0.04633025  0.05319435 -0.03884697  0.01743765  0.00658451  0.09437298]], new_loss = 0.003922959987690371\n",
            "step = 42, curr_theta = [[ 0.01257171  0.09121918  0.08451405  0.07202657  0.08288169  0.07140912\n",
            "   0.04633025  0.05319435 -0.03884697  0.01743765  0.00658451  0.09437298]], new_theta=[[ 0.01168507  0.09251829  0.08578651  0.07283676  0.08408013  0.0717837\n",
            "   0.04688792  0.053785   -0.04008464  0.01727853  0.00670735  0.09510221]], new_loss = 0.0038328330886408623\n",
            "step = 43, curr_theta = [[ 0.01168507  0.09251829  0.08578651  0.07283676  0.08408013  0.0717837\n",
            "   0.04688792  0.053785   -0.04008464  0.01727853  0.00670735  0.09510221]], new_theta=[[ 0.01081618  0.09379915  0.08704025  0.07363416  0.08526176  0.0721513\n",
            "   0.04743694  0.05436775 -0.04130025  0.01712052  0.00683172  0.09580252]], new_loss = 0.003745732555820913\n",
            "step = 44, curr_theta = [[ 0.01081618  0.09379915  0.08704025  0.07363416  0.08526176  0.0721513\n",
            "   0.04743694  0.05436775 -0.04130025  0.01712052  0.00683172  0.09580252]], new_theta=[[ 0.00996468  0.09506208  0.08827561  0.07441899  0.08642687  0.07251204\n",
            "   0.04797746  0.05494274 -0.0424942   0.01696366  0.00695756  0.09647469]], new_loss = 0.003661543721255184\n",
            "step = 45, curr_theta = [[ 0.00996468  0.09506208  0.08827561  0.07441899  0.08642687  0.07251204\n",
            "   0.04797746  0.05494274 -0.0424942   0.01696366  0.00695756  0.09647469]], new_theta=[[ 0.00913022  0.09630741  0.08949293  0.07519147  0.08757575  0.07286602\n",
            "   0.04850963  0.05551009 -0.04366687  0.01680798  0.00708479  0.09711947]], new_loss = 0.003580156576418739\n",
            "step = 46, curr_theta = [[ 0.00913022  0.09630741  0.08949293  0.07519147  0.08757575  0.07286602\n",
            "   0.04850963  0.05551009 -0.04366687  0.01680798  0.00708479  0.09711947]], new_theta=[[ 0.00831246  0.09753545  0.09069254  0.07595182  0.08870869  0.07321333\n",
            "   0.0490336   0.0560699  -0.04481866  0.01665352  0.00721336  0.0977376 ]], new_loss = 0.003501465575175247\n",
            "step = 47, curr_theta = [[ 0.00831246  0.09753545  0.09069254  0.07595182  0.08870869  0.07321333\n",
            "   0.0490336   0.0560699  -0.04481866  0.01665352  0.00721336  0.0977376 ]], new_theta=[[ 0.00751106  0.09874651  0.09187477  0.07670025  0.08982598  0.07355407\n",
            "   0.04954951  0.0566223  -0.04594992  0.01650031  0.00734321  0.0983298 ]], new_loss = 0.00342536944527197\n",
            "step = 48, curr_theta = [[ 0.00751106  0.09874651  0.09187477  0.07670025  0.08982598  0.07355407\n",
            "   0.04954951  0.0566223  -0.04594992  0.01650031  0.00734321  0.0983298 ]], new_theta=[[ 0.00672568  0.09994088  0.09303993  0.07743697  0.09092788  0.07388833\n",
            "   0.0500575   0.0571674  -0.04706105  0.01634838  0.00747429  0.09889675]], new_loss = 0.003351771007999677\n",
            "step = 49, curr_theta = [[ 0.00672568  0.09994088  0.09303993  0.07743697  0.09092788  0.07388833\n",
            "   0.0500575   0.0571674  -0.04706105  0.01634838  0.00747429  0.09889675]], new_theta=[[ 0.00595599  0.10111886  0.09418833  0.07816217  0.09201467  0.07421619\n",
            "   0.05055772  0.05770531 -0.04815239  0.01619774  0.00760654  0.09943914]], new_loss = 0.0032805770056497357\n",
            "step = 50, curr_theta = [[ 0.00595599  0.10111886  0.09418833  0.07816217  0.09201467  0.07421619\n",
            "   0.05055772  0.05770531 -0.04815239  0.01619774  0.00760654  0.09943914]], new_theta=[[ 0.00520166  0.10228074  0.09532028  0.07887605  0.0930866   0.07453776\n",
            "   0.05105029  0.05823613 -0.04922432  0.01604843  0.0077399   0.09995764]], new_loss = 0.0032116979364211957\n",
            "step = 51, curr_theta = [[ 0.00520166  0.10228074  0.09532028  0.07887605  0.0930866   0.07453776\n",
            "   0.05105029  0.05823613 -0.04922432  0.01604843  0.0077399   0.09995764]], new_theta=[[ 0.00446238  0.10342679  0.09643607  0.07957881  0.09414394  0.0748531\n",
            "   0.05153534  0.05875997 -0.05027717  0.01590046  0.00787432  0.1004529 ]], new_loss = 0.0031450478964485276\n",
            "step = 52, curr_theta = [[ 0.00446238  0.10342679  0.09643607  0.07957881  0.09414394  0.0748531\n",
            "   0.05153534  0.05875997 -0.05027717  0.01590046  0.00787432  0.1004529 ]], new_theta=[[ 0.00373783  0.1045573   0.097536    0.08027063  0.09518694  0.07516232\n",
            "   0.05201302  0.05927694 -0.0513113   0.01575386  0.00800976  0.10092553]], new_loss = 0.003080544428636803\n",
            "step = 53, curr_theta = [[ 0.00373783  0.1045573   0.097536    0.08027063  0.09518694  0.07516232\n",
            "   0.05201302  0.05927694 -0.0513113   0.01575386  0.00800976  0.10092553]], new_theta=[[ 0.0030277   0.10567253  0.09862035  0.08095169  0.09621585  0.07546548\n",
            "   0.05248343  0.05978713 -0.05232705  0.01560865  0.00814616  0.10137617]], new_loss = 0.003018108378005799\n",
            "step = 54, curr_theta = [[ 0.0030277   0.10567253  0.09862035  0.08095169  0.09621585  0.07546548\n",
            "   0.05248343  0.05978713 -0.05232705  0.01560865  0.00814616  0.10137617]], new_theta=[[ 0.0023317   0.10677275  0.0996894   0.08162219  0.09723091  0.07576267\n",
            "   0.05294671  0.06029065 -0.05332475  0.01546484  0.00828347  0.1018054 ]], new_loss = 0.0029576637532582883\n",
            "step = 55, curr_theta = [[ 0.0023317   0.10677275  0.0996894   0.08162219  0.09723091  0.07576267\n",
            "   0.05294671  0.06029065 -0.05332475  0.01546484  0.00828347  0.1018054 ]], new_theta=[[ 0.00164953  0.10785821  0.10074342  0.08228229  0.09823235  0.07605397\n",
            "   0.05340298  0.06078758 -0.05430472  0.01532245  0.00842164  0.10221382]], new_loss = 0.0028991375943006313\n",
            "step = 56, curr_theta = [[ 0.00164953  0.10785821  0.10074342  0.08228229  0.09823235  0.07605397\n",
            "   0.05340298  0.06078758 -0.05430472  0.01532245  0.00842164  0.10221382]], new_theta=[[ 0.0009809   0.10892918  0.10178269  0.08293218  0.09922042  0.07633945\n",
            "   0.05385235  0.06127803 -0.05526728  0.01518149  0.00856064  0.102602  ]], new_loss = 0.0028424598454559057\n",
            "step = 57, curr_theta = [[ 0.0009809   0.10892918  0.10178269  0.08293218  0.09922042  0.07633945\n",
            "   0.05385235  0.06127803 -0.05526728  0.01518149  0.00856064  0.102602  ]], new_theta=[[ 0.00032552  0.10998591  0.10280747  0.08357201  0.10019533  0.07661921\n",
            "   0.05429494  0.0617621  -0.05621276  0.01504199  0.00870041  0.1029705 ]], new_loss = 0.002787563234121424\n",
            "step = 58, curr_theta = [[ 0.00032552  0.10998591  0.10280747  0.08357201  0.10019533  0.07661921\n",
            "   0.05429494  0.0617621  -0.05621276  0.01504199  0.00870041  0.1029705 ]], new_theta=[[-0.00031687  0.11102863  0.10381801  0.08420196  0.10115732  0.0768933\n",
            "   0.05473088  0.06223987 -0.05714145  0.01490394  0.00884092  0.10331985]], new_loss = 0.0027343831546334054\n",
            "step = 59, curr_theta = [[-0.00031687  0.11102863  0.10381801  0.08420196  0.10115732  0.0768933\n",
            "   0.05473088  0.06223987 -0.05714145  0.01490394  0.00884092  0.10331985]], new_theta=[[-0.00094656  0.11205758  0.10481457  0.08482219  0.10210661  0.07716182\n",
            "   0.05516026  0.06271143 -0.05805366  0.01476737  0.00898211  0.10365059]], new_loss = 0.002682857557112059\n",
            "step = 60, curr_theta = [[-0.00094656  0.11205758  0.10481457  0.08482219  0.10210661  0.07716182\n",
            "   0.05516026  0.06271143 -0.05805366  0.01476737  0.00898211  0.10365059]], new_theta=[[-0.0015638   0.11307301  0.1057974   0.08543287  0.1030434   0.07742482\n",
            "   0.0555832   0.06317687 -0.05894968  0.01463229  0.00912396  0.10396324]], new_loss = 0.002632926841070292\n",
            "step = 61, curr_theta = [[-0.0015638   0.11307301  0.1057974   0.08543287  0.1030434   0.07742482\n",
            "   0.0555832   0.06317687 -0.05894968  0.01463229  0.00912396  0.10396324]], new_theta=[[-0.00216886  0.11407514  0.10676674  0.08603415  0.10396792  0.07768239\n",
            "   0.05599981  0.06363629 -0.05982981  0.01449869  0.00926641  0.10425831]], new_loss = 0.0025845337535787983\n",
            "step = 62, curr_theta = [[-0.00216886  0.11407514  0.10676674  0.08603415  0.10396792  0.07768239\n",
            "   0.05599981  0.06363629 -0.05982981  0.01449869  0.00926641  0.10425831]], new_theta=[[-0.002762    0.1150642   0.10772282  0.08662618  0.10488036  0.0779346\n",
            "   0.0564102   0.06408977 -0.06069433  0.01436661  0.00940944  0.10453628]], new_loss = 0.0025376232917893892\n",
            "step = 63, curr_theta = [[-0.002762    0.1150642   0.10772282  0.08662618  0.10488036  0.0779346\n",
            "   0.0564102   0.06408977 -0.06069433  0.01436661  0.00940944  0.10453628]], new_theta=[[-0.00334345  0.11604042  0.10866589  0.08720912  0.10578094  0.07818152\n",
            "   0.05681446  0.06453738 -0.06154352  0.01423604  0.009553    0.10479764]], new_loss = 0.002492142609627086\n",
            "step = 64, curr_theta = [[-0.00334345  0.11604042  0.10866589  0.08720912  0.10578094  0.07818152\n",
            "   0.05681446  0.06453738 -0.06154352  0.01423604  0.009553    0.10479764]], new_theta=[[-0.00391347  0.117004    0.10959617  0.08778312  0.10666985  0.07842323\n",
            "   0.0572127   0.06497923 -0.06237766  0.01410698  0.00969706  0.10504285]], new_loss = 0.002448040928469839\n",
            "step = 65, curr_theta = [[-0.00391347  0.117004    0.10959617  0.08778312  0.10666985  0.07842323\n",
            "   0.0572127   0.06497923 -0.06237766  0.01410698  0.00969706  0.10504285]], new_theta=[[-0.00447229  0.11795516  0.11051389  0.08834832  0.10754729  0.07865978\n",
            "   0.05760502  0.06541538 -0.063197    0.01397946  0.00984158  0.10527237]], new_loss = 0.0024052694516426654\n",
            "step = 66, curr_theta = [[-0.00447229  0.11795516  0.11051389  0.08834832  0.10754729  0.07865978\n",
            "   0.05760502  0.06541538 -0.063197    0.01397946  0.00984158  0.10527237]], new_theta=[[-0.00502016  0.11889411  0.11141926  0.08890486  0.10841345  0.07889126\n",
            "   0.05799152  0.06584592 -0.06400182  0.01385346  0.00998652  0.10548666]], new_loss = 0.0023637812825605723\n",
            "step = 67, curr_theta = [[-0.00502016  0.11889411  0.11141926  0.08890486  0.10841345  0.07889126\n",
            "   0.05799152  0.06584592 -0.06400182  0.01385346  0.00998652  0.10548666]], new_theta=[[-0.00555728  0.11982107  0.11231251  0.0894529   0.10926851  0.07911772\n",
            "   0.05837229  0.06627093 -0.06479238  0.01372901  0.01013187  0.10568614]], new_loss = 0.0023235313463619233\n",
            "step = 68, curr_theta = [[-0.00555728  0.11982107  0.11231251  0.0894529   0.10926851  0.07911772\n",
            "   0.05837229  0.06627093 -0.06479238  0.01372901  0.01013187  0.10568614]], new_theta=[[-0.0060839   0.12073622  0.11319384  0.08999256  0.11011268  0.07933924\n",
            "   0.05874743  0.06669048 -0.06556892  0.01360609  0.01027757  0.10587124]], new_loss = 0.002284476314880795\n",
            "step = 69, curr_theta = [[-0.0060839   0.12073622  0.11319384  0.08999256  0.11011268  0.07933924\n",
            "   0.05874743  0.06669048 -0.06556892  0.01360609  0.01027757  0.10587124]], new_theta=[[-0.00660023  0.12163976  0.11406346  0.09052398  0.11094611  0.07955589\n",
            "   0.05911703  0.06710465 -0.0663317   0.01348472  0.01042361  0.10604237]], new_loss = 0.0022465745348135492\n",
            "step = 70, curr_theta = [[-0.00660023  0.12163976  0.11406346  0.09052398  0.11094611  0.07955589\n",
            "   0.05911703  0.06710465 -0.0663317   0.01348472  0.01042361  0.10604237]], new_theta=[[-0.00710647  0.1225319   0.11492157  0.09104729  0.111769    0.07976772\n",
            "   0.05948118  0.06751353 -0.06708096  0.01336491  0.01056994  0.10619994]], new_loss = 0.0022097859589411384\n",
            "step = 71, curr_theta = [[-0.00710647  0.1225319   0.11492157  0.09104729  0.111769    0.07976772\n",
            "   0.05948118  0.06751353 -0.06708096  0.01336491  0.01056994  0.10619994]], new_theta=[[-0.00760285  0.12341281  0.11576838  0.09156263  0.11258151  0.07997481\n",
            "   0.05983998  0.06791717 -0.06781694  0.01324664  0.01071655  0.10634434]], new_loss = 0.0021740720802747595\n",
            "step = 72, curr_theta = [[-0.00760285  0.12341281  0.11576838  0.09156263  0.11258151  0.07997481\n",
            "   0.05983998  0.06791717 -0.06781694  0.01324664  0.01071655  0.10634434]], new_theta=[[-0.00808957  0.12428269  0.11660407  0.09207012  0.11338382  0.08017722\n",
            "   0.0601935   0.06831566 -0.06853987  0.01312992  0.0108634   0.10647596]], new_loss = 0.002139395868998241\n",
            "step = 73, curr_theta = [[-0.00808957  0.12428269  0.11660407  0.09207012  0.11338382  0.08017722\n",
            "   0.0601935   0.06831566 -0.06853987  0.01312992  0.0108634   0.10647596]], new_theta=[[-0.00856683  0.12514173  0.11742885  0.09256989  0.1141761   0.08037502\n",
            "   0.06054183  0.06870907 -0.06924998  0.01301476  0.01101047  0.10659517]], new_loss = 0.002105721712086096\n",
            "step = 74, curr_theta = [[-0.00856683  0.12514173  0.11742885  0.09256989  0.1141761   0.08037502\n",
            "   0.06054183  0.06870907 -0.06924998  0.01301476  0.01101047  0.10659517]], new_theta=[[-0.00903482  0.12599009  0.1182429   0.09306206  0.11495851  0.08056826\n",
            "   0.06088507  0.06909746 -0.06994751  0.01290116  0.01115772  0.10670233]], new_loss = 0.002073015355481469\n",
            "step = 75, curr_theta = [[-0.00903482  0.12599009  0.1182429   0.09306206  0.11495851  0.08056826\n",
            "   0.06088507  0.06909746 -0.06994751  0.01290116  0.01115772  0.10670233]], new_theta=[[-0.00949374  0.12682795  0.1190464   0.09354676  0.11573121  0.08075701\n",
            "   0.06122328  0.06948092 -0.07063266  0.01278911  0.01130514  0.1067978 ]], new_loss = 0.0020412438487232604\n",
            "step = 76, curr_theta = [[-0.00949374  0.12682795  0.1190464   0.09354676  0.11573121  0.08075701\n",
            "   0.06122328  0.06948092 -0.07063266  0.01278911  0.01130514  0.1067978 ]], new_theta=[[-0.00994377  0.12765549  0.11983953  0.0940241   0.11649437  0.08094132\n",
            "   0.06155656  0.06985949 -0.07130566  0.01267861  0.01145269  0.10688194]], new_loss = 0.0020103754919165388\n",
            "step = 77, curr_theta = [[-0.00994377  0.12765549  0.11983953  0.0940241   0.11649437  0.08094132\n",
            "   0.06155656  0.06985949 -0.07130566  0.01267861  0.01145269  0.10688194]], new_theta=[[-0.01038511  0.12847288  0.12062247  0.0944942   0.11724812  0.08112127\n",
            "   0.06188498  0.07023326 -0.07196671  0.01256967  0.01160036  0.10695507]], new_loss = 0.001980379784945036\n",
            "step = 78, curr_theta = [[-0.01038511  0.12847288  0.12062247  0.0944942   0.11724812  0.08112127\n",
            "   0.06188498  0.07023326 -0.07196671  0.01256967  0.01160036  0.10695507]], new_theta=[[-0.01081792  0.12928029  0.1213954   0.09495717  0.11799264  0.08129691\n",
            "   0.06220862  0.07060229 -0.07261604  0.01246228  0.01174811  0.10701753]], new_loss = 0.0019512273788288516\n",
            "step = 79, curr_theta = [[-0.01081792  0.12928029  0.1213954   0.09495717  0.11799264  0.08129691\n",
            "   0.06220862  0.07060229 -0.07261604  0.01246228  0.01174811  0.10701753]], new_theta=[[-0.01124239  0.13007787  0.12215849  0.09541313  0.11872807  0.08146831\n",
            "   0.06252755  0.07096664 -0.07325384  0.01235644  0.01189593  0.10706964]], new_loss = 0.0019228900291348055\n",
            "step = 80, curr_theta = [[-0.01124239  0.13007787  0.12215849  0.09541313  0.11872807  0.08146831\n",
            "   0.06252755  0.07096664 -0.07325384  0.01235644  0.01189593  0.10706964]], new_theta=[[-0.01165869  0.13086579  0.1229119   0.09586219  0.11945455  0.08163551\n",
            "   0.06284186  0.07132638 -0.07388031  0.01225215  0.01204379  0.10711172]], new_loss = 0.0018953405513508737\n",
            "step = 81, curr_theta = [[-0.01165869  0.13086579  0.1229119   0.09586219  0.11945455  0.08163551\n",
            "   0.06284186  0.07132638 -0.07388031  0.01225215  0.01204379  0.10711172]], new_theta=[[-0.01206698  0.1316442   0.1236558   0.09630445  0.12017224  0.08179857\n",
            "   0.06315162  0.07168157 -0.07449565  0.0121494   0.01219168  0.10714407]], new_loss = 0.0018685527781400152\n",
            "step = 82, curr_theta = [[-0.01206698  0.1316442   0.1236558   0.09630445  0.12017224  0.08179857\n",
            "   0.06315162  0.07168157 -0.07449565  0.0121494   0.01219168  0.10714407]], new_theta=[[-0.01246744  0.13241327  0.12439034  0.09674003  0.12088127  0.08195756\n",
            "   0.06345689  0.07203227 -0.07510006  0.01204819  0.01233956  0.10716699]], new_loss = 0.0018425015183924185\n",
            "step = 83, curr_theta = [[-0.01246744  0.13241327  0.12439034  0.09674003  0.12088127  0.08195756\n",
            "   0.06345689  0.07203227 -0.07510006  0.01204819  0.01233956  0.10716699]], new_theta=[[-0.01286022  0.13317313  0.12511569  0.09716902  0.12158177  0.08211254\n",
            "   0.06375775  0.07237854 -0.07569372  0.01194852  0.01248743  0.10718077]], new_loss = 0.0018171625179986828\n",
            "step = 84, curr_theta = [[-0.01286022  0.13317313  0.12511569  0.09716902  0.12158177  0.08211254\n",
            "   0.06375775  0.07237854 -0.07569372  0.01194852  0.01248743  0.10718077]], new_theta=[[-0.01324549  0.13392394  0.125832    0.09759154  0.1222739   0.08226355\n",
            "   0.06405428  0.07272045 -0.07627682  0.01185038  0.01263526  0.10718569]], new_loss = 0.0017925124222698875\n",
            "step = 85, curr_theta = [[-0.01324549  0.13392394  0.125832    0.09759154  0.1222739   0.08226355\n",
            "   0.06405428  0.07272045 -0.07627682  0.01185038  0.01263526  0.10718569]], new_theta=[[-0.01362339  0.13466584  0.12653942  0.09800767  0.12295778  0.08241065\n",
            "   0.06434653  0.07305805 -0.07684954  0.01175377  0.01278303  0.10718204]], new_loss = 0.001768528739933658\n",
            "step = 86, curr_theta = [[-0.01362339  0.13466584  0.12653942  0.09800767  0.12295778  0.08241065\n",
            "   0.06434653  0.07305805 -0.07684954  0.01175377  0.01278303  0.10718204]], new_theta=[[-0.01399409  0.13539899  0.12723811  0.09841753  0.12363354  0.08255389\n",
            "   0.06463458  0.0733914  -0.07741206  0.01165868  0.01293072  0.10717007]], new_loss = 0.0017451898086385206\n",
            "step = 87, curr_theta = [[-0.01399409  0.13539899  0.12723811  0.09841753  0.12363354  0.08255389\n",
            "   0.06463458  0.0733914  -0.07741206  0.01165868  0.01293072  0.10717007]], new_theta=[[-0.01435772  0.13612351  0.1279282   0.0988212   0.12430131  0.08269334\n",
            "   0.06491849  0.07372056 -0.07796456  0.01156511  0.01307832  0.10715006]], new_loss = 0.00172247476190168\n",
            "step = 88, curr_theta = [[-0.01435772  0.13612351  0.1279282   0.0988212   0.12430131  0.08269334\n",
            "   0.06491849  0.07372056 -0.07796456  0.01156511  0.01307832  0.10715006]], new_theta=[[-0.01471443  0.13683954  0.12860984  0.09921878  0.12496122  0.08282904\n",
            "   0.06519833  0.07404559 -0.07850721  0.01147305  0.01322581  0.10712226]], new_loss = 0.001700363497438292\n",
            "step = 89, curr_theta = [[-0.01471443  0.13683954  0.12860984  0.09921878  0.12496122  0.08282904\n",
            "   0.06519833  0.07404559 -0.07850721  0.01147305  0.01322581  0.10712226]], new_theta=[[-0.01506437  0.13754723  0.12928317  0.09961037  0.12561339  0.08296105\n",
            "   0.06547416  0.07436653 -0.07904017  0.0113825   0.01337318  0.10708692]], new_loss = 0.0016788366468128803\n",
            "step = 90, curr_theta = [[-0.01506437  0.13754723  0.12928317  0.09961037  0.12561339  0.08296105\n",
            "   0.06547416  0.07436653 -0.07904017  0.0113825   0.01337318  0.10708692]], new_theta=[[-0.01540768  0.13824671  0.12994833  0.09999606  0.12625795  0.08308942\n",
            "   0.06574604  0.07468345 -0.07956362  0.01129344  0.0135204   0.10704428]], new_loss = 0.0016578755463562628\n",
            "step = 91, curr_theta = [[-0.01540768  0.13824671  0.12994833  0.09999606  0.12625795  0.08308942\n",
            "   0.06574604  0.07468345 -0.07956362  0.01129344  0.0135204   0.10704428]], new_theta=[[-0.01574448  0.13893809  0.13060545  0.10037594  0.12689501  0.08321419\n",
            "   0.06601404  0.0749964  -0.08007772  0.01120588  0.01366746  0.10699459]], new_loss = 0.0016374622092936893\n",
            "step = 92, curr_theta = [[-0.01574448  0.13893809  0.13060545  0.10037594  0.12689501  0.08321419\n",
            "   0.06601404  0.0749964  -0.08007772  0.01120588  0.01366746  0.10699459]], new_theta=[[-0.01607492  0.13962152  0.13125467  0.1007501   0.1275247   0.08333543\n",
            "   0.06627821  0.07530542 -0.08058262  0.01111981  0.01381435  0.10693806]], new_loss = 0.0016175792990323484\n",
            "step = 93, curr_theta = [[-0.01607492  0.13962152  0.13125467  0.1007501   0.1275247   0.08333543\n",
            "   0.06627821  0.07530542 -0.08058262  0.01111981  0.01381435  0.10693806]], new_theta=[[-0.01639913  0.14029712  0.13189612  0.10111862  0.12814712  0.08345317\n",
            "   0.06653862  0.07561058 -0.08107849  0.01103521  0.01396105  0.10687493]], new_loss = 0.0015982101035586025\n",
            "step = 94, curr_theta = [[-0.01639913  0.14029712  0.13189612  0.10111862  0.12814712  0.08345317\n",
            "   0.06653862  0.07561058 -0.08107849  0.01103521  0.01396105  0.10687493]], new_theta=[[-0.01671722  0.140965    0.13252992  0.1014816   0.12876239  0.08356748\n",
            "   0.06679532  0.07591193 -0.08156547  0.01095209  0.01410754  0.10680541]], new_loss = 0.0015793385108975\n",
            "step = 95, curr_theta = [[-0.01671722  0.140965    0.13252992  0.1014816   0.12876239  0.08356748\n",
            "   0.06679532  0.07591193 -0.08156547  0.01095209  0.01410754  0.10680541]], new_theta=[[-0.01702934  0.14162531  0.1331562   0.10183911  0.12937063  0.0836784\n",
            "   0.06704837  0.0762095  -0.08204372  0.01087043  0.01425383  0.10672972]], new_loss = 0.0015609489855891473\n",
            "step = 96, curr_theta = [[-0.01702934  0.14162531  0.1331562   0.10183911  0.12937063  0.0836784\n",
            "   0.06704837  0.0762095  -0.08204372  0.01087043  0.01425383  0.10672972]], new_theta=[[-0.01733559  0.14227814  0.13377509  0.10219123  0.12997194  0.08378597\n",
            "   0.06729783  0.07650337 -0.08251339  0.01079022  0.01439988  0.10664807]], new_loss = 0.0015430265461385156\n",
            "step = 97, curr_theta = [[-0.01733559  0.14227814  0.13377509  0.10219123  0.12997194  0.08378597\n",
            "   0.06729783  0.07650337 -0.08251339  0.01079022  0.01439988  0.10664807]], new_theta=[[-0.01763611  0.14292362  0.13438669  0.10253806  0.13056642  0.08389025\n",
            "   0.06754375  0.07679356 -0.08297462  0.01071147  0.0145457   0.10656066]], new_loss = 0.0015255567433971502\n",
            "step = 98, curr_theta = [[-0.01763611  0.14292362  0.13438669  0.10253806  0.13056642  0.08389025\n",
            "   0.06754375  0.07679356 -0.08297462  0.01071147  0.0145457   0.10656066]], new_theta=[[-0.017931    0.14356186  0.13499114  0.10287967  0.13115419  0.08399128\n",
            "   0.06778618  0.07708014 -0.08342756  0.01063415  0.01469126  0.10646769]], new_loss = 0.0015085256398370097\n",
            "step = 99, curr_theta = [[-0.017931    0.14356186  0.13499114  0.10287967  0.13115419  0.08399128\n",
            "   0.06778618  0.07708014 -0.08342756  0.01063415  0.01469126  0.10646769]], new_theta=[[-0.01822038  0.14419297  0.13558855  0.10321613  0.13173535  0.08408911\n",
            "   0.06802519  0.07736314 -0.08387234  0.01055826  0.01483656  0.10636935]], new_loss = 0.0014919197896784537\n",
            "step = 100, curr_theta = [[-0.01822038  0.14419297  0.13558855  0.10321613  0.13173535  0.08408911\n",
            "   0.06802519  0.07736314 -0.08387234  0.01055826  0.01483656  0.10636935]], new_theta=[[-0.01850436  0.14481708  0.13617903  0.10354754  0.13230999  0.08418378\n",
            "   0.06826081  0.07764262 -0.0843091   0.01048379  0.01498158  0.10626583]], new_loss = 0.001475726219835978\n",
            "step = 101, curr_theta = [[-0.01850436  0.14481708  0.13617903  0.10354754  0.13230999  0.08418378\n",
            "   0.06826081  0.07764262 -0.0843091   0.01048379  0.01498158  0.10626583]], new_theta=[[-0.01878306  0.14543427  0.13676269  0.10387395  0.13287822  0.08427535\n",
            "   0.06849311  0.07791862 -0.08473798  0.01041073  0.01512632  0.10615731]], new_loss = 0.001459932411646935\n",
            "step = 102, curr_theta = [[-0.01878306  0.14543427  0.13676269  0.10387395  0.13287822  0.08427535\n",
            "   0.06849311  0.07791862 -0.08473798  0.01041073  0.01512632  0.10615731]], new_theta=[[-0.01905659  0.14604467  0.13733964  0.10419544  0.13344014  0.08436385\n",
            "   0.06872213  0.07819119 -0.08515911  0.01033908  0.01527076  0.10604397]], new_loss = 0.001444526283349923\n",
            "step = 103, curr_theta = [[-0.01905659  0.14604467  0.13733964  0.10419544  0.13344014  0.08436385\n",
            "   0.06872213  0.07819119 -0.08515911  0.01033908  0.01527076  0.10604397]], new_theta=[[-0.01932503  0.14664837  0.13791     0.1045121   0.13399584  0.08444933\n",
            "   0.06894792  0.07846037 -0.08557262  0.01026882  0.01541489  0.10592597]], new_loss = 0.0014294961732810253\n",
            "step = 104, curr_theta = [[-0.01932503  0.14664837  0.13791     0.1045121   0.13399584  0.08444933\n",
            "   0.06894792  0.07846037 -0.08557262  0.01026882  0.01541489  0.10592597]], new_theta=[[-0.01958851  0.14724548  0.13847387  0.10482399  0.13454542  0.08453183\n",
            "   0.06917053  0.0787262  -0.08597863  0.01019994  0.01555871  0.1058035 ]], new_loss = 0.0014148308237574288\n",
            "step = 105, curr_theta = [[-0.01958851  0.14724548  0.13847387  0.10482399  0.13454542  0.08453183\n",
            "   0.06917053  0.0787262  -0.08597863  0.01019994  0.01555871  0.1058035 ]], new_theta=[[-0.01984712  0.1478361   0.13903134  0.10513117  0.13508897  0.0846114\n",
            "   0.06939002  0.07898873 -0.08637728  0.01013244  0.01570221  0.10567672]], new_loss = 0.00140051936561926\n",
            "step = 106, curr_theta = [[-0.01984712  0.1478361   0.13903134  0.10513117  0.13508897  0.0846114\n",
            "   0.06939002  0.07898873 -0.08637728  0.01013244  0.01570221  0.10567672]], new_theta=[[-0.02010096  0.14842032  0.13958253  0.10543373  0.13562658  0.08468809\n",
            "   0.06960641  0.079248   -0.08676869  0.01006631  0.01584537  0.10554578]], new_loss = 0.0013865513034017833\n",
            "step = 107, curr_theta = [[-0.02010096  0.14842032  0.13958253  0.10543373  0.13562658  0.08468809\n",
            "   0.06960641  0.079248   -0.08676869  0.01006631  0.01584537  0.10554578]], new_theta=[[-0.02035012  0.14899825  0.14012753  0.10573173  0.13615834  0.08476192\n",
            "   0.06981977  0.07950405 -0.08715297  0.01000152  0.01598819  0.10541084]], new_loss = 0.0013729165011112437\n",
            "step = 108, curr_theta = [[-0.02035012  0.14899825  0.14012753  0.10573173  0.13615834  0.08476192\n",
            "   0.06981977  0.07950405 -0.08715297  0.01000152  0.01598819  0.10541084]], new_theta=[[-0.02059471  0.14956998  0.14066644  0.10602523  0.13668433  0.08483295\n",
            "   0.07003014  0.07975692 -0.08753024  0.00993808  0.01613066  0.10527205]], new_loss = 0.001359605168578861\n",
            "step = 109, curr_theta = [[-0.02059471  0.14956998  0.14066644  0.10602523  0.13668433  0.08483295\n",
            "   0.07003014  0.07975692 -0.08753024  0.00993808  0.01613066  0.10527205]], new_theta=[[-0.02083481  0.1501356   0.14119935  0.1063143   0.13720465  0.08490121\n",
            "   0.07023755  0.08000666 -0.08790063  0.00987598  0.01627277  0.10512957]], new_loss = 0.0013466078483685283\n",
            "step = 110, curr_theta = [[-0.02083481  0.1501356   0.14119935  0.1063143   0.13720465  0.08490121\n",
            "   0.07023755  0.08000666 -0.08790063  0.00987598  0.01627277  0.10512957]], new_theta=[[-0.02107051  0.15069521  0.14172637  0.10659901  0.13771938  0.08496675\n",
            "   0.07044207  0.08025331 -0.08826424  0.0098152   0.01641451  0.10498352]], new_loss = 0.0013339154032148456\n",
            "step = 111, curr_theta = [[-0.02107051  0.15069521  0.14172637  0.10659901  0.13771938  0.08496675\n",
            "   0.07044207  0.08025331 -0.08826424  0.0098152   0.01641451  0.10498352]], new_theta=[[-0.02130191  0.1512489   0.14224758  0.10687942  0.1382286   0.0850296\n",
            "   0.07064372  0.08049689 -0.08862119  0.00975573  0.01655588  0.10483406]], new_loss = 0.0013215190039691453\n",
            "step = 112, curr_theta = [[-0.02130191  0.1512489   0.14224758  0.10687942  0.1382286   0.0850296\n",
            "   0.07064372  0.08049689 -0.08862119  0.00975573  0.01655588  0.10483406]], new_theta=[[-0.02152909  0.15179675  0.14276308  0.10715559  0.1387324   0.08508981\n",
            "   0.07084255  0.08073746 -0.08897159  0.00969756  0.01669688  0.10468133]], new_loss = 0.001309410118032082\n",
            "step = 113, curr_theta = [[-0.02152909  0.15179675  0.14276308  0.10715559  0.1387324   0.08508981\n",
            "   0.07084255  0.08073746 -0.08897159  0.00969756  0.01669688  0.10468133]], new_theta=[[-0.02175213  0.15233886  0.14327295  0.10742759  0.13923085  0.08514742\n",
            "   0.0710386   0.08097505 -0.08931554  0.00964069  0.01683748  0.10452544]], new_loss = 0.0012975804982523328\n",
            "step = 114, curr_theta = [[-0.02175213  0.15233886  0.14327295  0.10742759  0.13923085  0.08514742\n",
            "   0.0710386   0.08097505 -0.08931554  0.00964069  0.01683748  0.10452544]], new_theta=[[-0.02197112  0.1528753   0.14377729  0.10769547  0.13972403  0.08520245\n",
            "   0.07123191  0.0812097  -0.08965315  0.00958509  0.0169777   0.10436654]], new_loss = 0.0012860221722718092\n",
            "step = 115, curr_theta = [[-0.02197112  0.1528753   0.14377729  0.10769547  0.13972403  0.08520245\n",
            "   0.07123191  0.0812097  -0.08965315  0.00958509  0.0169777   0.10436654]], new_theta=[[-0.02218615  0.15340617  0.14427617  0.10795929  0.14021203  0.08525497\n",
            "   0.07142253  0.08144144 -0.08998454  0.00953077  0.01711752  0.10420474]], new_loss = 0.0012747274322986218\n",
            "step = 116, curr_theta = [[-0.02218615  0.15340617  0.14427617  0.10795929  0.14021203  0.08525497\n",
            "   0.07142253  0.08144144 -0.08998454  0.00953077  0.01711752  0.10420474]], new_theta=[[-0.02239728  0.15393154  0.14476969  0.10821912  0.14069491  0.08530499\n",
            "   0.07161049  0.08167031 -0.09030979  0.0094777   0.01725693  0.10404017]], new_loss = 0.0012636888252898625\n",
            "step = 117, curr_theta = [[-0.02239728  0.15393154  0.14476969  0.10821912  0.14069491  0.08530499\n",
            "   0.07161049  0.08167031 -0.09030979  0.0094777   0.01725693  0.10404017]], new_theta=[[-0.0226046   0.1544515   0.14525792  0.108475    0.14117275  0.08535255\n",
            "   0.07179583  0.08189635 -0.09062901  0.00942588  0.01739593  0.10387296]], new_loss = 0.0012528991435270264\n",
            "step = 118, curr_theta = [[-0.0226046   0.1544515   0.14525792  0.108475    0.14117275  0.08535255\n",
            "   0.07179583  0.08189635 -0.09062901  0.00942588  0.01739593  0.10387296]], new_theta=[[-0.02280819  0.15496613  0.14574095  0.10872701  0.14164563  0.08539771\n",
            "   0.07197859  0.0821196  -0.09094229  0.0093753   0.01753452  0.1037032 ]], new_loss = 0.0012423514155676506\n",
            "step = 119, curr_theta = [[-0.02280819  0.15496613  0.14574095  0.10872701  0.14164563  0.08539771\n",
            "   0.07197859  0.0821196  -0.09094229  0.0093753   0.01753452  0.1037032 ]], new_theta=[[-0.02300811  0.15547551  0.14621886  0.10897518  0.14211362  0.08544048\n",
            "   0.0721588   0.08234008 -0.09124975  0.00932595  0.01767269  0.10353102]], new_loss = 0.0012320388975574157\n",
            "step = 120, curr_theta = [[-0.02300811  0.15547551  0.14621886  0.10897518  0.14211362  0.08544048\n",
            "   0.0721588   0.08234008 -0.09124975  0.00932595  0.01767269  0.10353102]], new_theta=[[-0.02320445  0.1559797   0.14669172  0.10921958  0.14257678  0.08548091\n",
            "   0.07233651  0.08255783 -0.09155146  0.00927781  0.01781044  0.10335653]], new_loss = 0.0012219550648876882\n",
            "step = 121, curr_theta = [[-0.02320445  0.1559797   0.14669172  0.10921958  0.14257678  0.08548091\n",
            "   0.07233651  0.08255783 -0.09155146  0.00927781  0.01781044  0.10335653]], new_theta=[[-0.02339728  0.1564788   0.14715962  0.10946026  0.1430352   0.08551903\n",
            "   0.07251175  0.0827729  -0.09184752  0.00923087  0.01794776  0.10317983]], new_loss = 0.0012120936041840745\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7efbe1ea6910>]"
            ]
          },
          "metadata": {},
          "execution_count": 165
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen, our cost function is almost 0."
      ],
      "metadata": {
        "id": "bUvSSpB2n_54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(); plt.plot(final_theta.flatten())"
      ],
      "metadata": {
        "id": "y94y7Nuyq--Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "6b94105b-a20f-4bac-977f-7c5c4f95fdfb"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7efbe1e105d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 166
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(final_theta))\n",
        "print(final_theta)\n",
        "print(np.argmax(np.abs(final_theta)))\n",
        "print(np.argmin(np.abs(final_theta)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK2af48OiTCD",
        "outputId": "e1f7ee21-02d8-4292-b260-05be7ab6a771"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 12)\n",
            "[[-0.02339728  0.1564788   0.14715962  0.10946026  0.1430352   0.08551903\n",
            "   0.07251175  0.0827729  -0.09184752  0.00923087  0.01794776  0.10317983]]\n",
            "1\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see which feauture is more significant we have the plot above. It shows that our first and second features are more important. our 9th feautre is not that much important."
      ],
      "metadata": {
        "id": "H6fhC86JoJKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#We can go with just the two features that are more influential than other, but as long as other features have influenced our data, the error value will increase if we ignore other features."
      ],
      "metadata": {
        "id": "wlDa-rzlohYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_test = np.shape(y_test)[1]\n",
        "x_test_bias = np.vstack((np.ones((1, num_test)), x_test)) \n",
        "y_predict_test = np.matmul(final_theta, x_test_bias)"
      ],
      "metadata": {
        "id": "UOr4geVmrEdY"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_error = (y_predict_test - y_test)\n",
        "print(np.sqrt(np.sum(np.square(test_error))) / 6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhCRFG_uhEg7",
        "outputId": "d6dbed7a-2bc3-4c52-b919-7005250f1a33"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.041220504974468615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The average least square errors is almost 4%."
      ],
      "metadata": {
        "id": "y3h1-1S4o2AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the least effect: Construction type\n",
        "The most effect: Local price and bathrooms"
      ],
      "metadata": {
        "id": "eBN09pd3pYJQ"
      }
    }
  ]
}